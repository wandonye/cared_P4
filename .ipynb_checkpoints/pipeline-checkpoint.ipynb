{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.cm as cm\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import operator\n",
    "import heapq\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "check_imgs = [\"camera_cal/calibration\"+str(i)+\".jpg\" for i in range(1,21)]\n",
    "grid_sizes = [(9,5),(9,6),(9,6),(6,5),(7,6)]+ [(9,6)]*15\n",
    "\n",
    "def calibration_matrix(imgs, grid_sizes):\n",
    "    objpoints = []\n",
    "    imgpoints = []\n",
    "    for idx, fname in enumerate(imgs):\n",
    "        img = cv2.imread(fname)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        grid_size = grid_sizes[idx]\n",
    "        # Find the chessboard corners\n",
    "        ret, corners = cv2.findChessboardCorners(gray, grid_size, None)\n",
    "\n",
    "        # If found, add object points, image points\n",
    "        if ret == True:\n",
    "            objp = np.zeros((grid_size[0]*grid_size[1],3), np.float32)\n",
    "            objp[:,:2] = np.mgrid[0:grid_size[0], 0:grid_size[1]].T.reshape(-1,2)\n",
    "\n",
    "            objpoints.append(objp)\n",
    "            imgpoints.append(corners)\n",
    "            # Draw and display the corners\n",
    "#             cv2.drawChessboardCorners(img, grid_size, corners, ret)\n",
    "#             write_name = 'camera_cal/corners_found'+str(idx+1)+'.jpg'\n",
    "#             cv2.imwrite(write_name, img)\n",
    "    #         cv2.imshow('img', img)\n",
    "    #         cv2.waitKey(500)\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "    return (mtx,dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mtx, dist = calibration_matrix(check_imgs,grid_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = (720,720)\n",
    "\n",
    "def perspective_transformation(target_img_size):\n",
    "    src = np.array([[592, 453], \n",
    "                    [695,453],\n",
    "                    [970, 630],\n",
    "                    [348, 630]], dtype = \"float32\")\n",
    "\n",
    "    h_margin = 200\n",
    "    top_margin = 200\n",
    "    bottom_margin = 50\n",
    "    dst = np.array([[h_margin, top_margin], \n",
    "                    [target_img_size[0] - h_margin, top_margin],\n",
    "                    [target_img_size[0] - h_margin, target_img_size[1] - bottom_margin],\n",
    "                    [h_margin, target_img_size[1] - bottom_margin]], dtype = \"float32\")\n",
    "    return cv2.getPerspectiveTransform(src, dst)\n",
    "\n",
    "M = perspective_transformation(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def topo_parabolic_bump_filter(img, x_thresh=(0.5,1), \n",
    "                               y_thresh=(0,0.0001), \n",
    "                               min_area=400, ksize=3):\n",
    "    kernel = np.array([[-1]*ksize+[2]*ksize+[-1]*ksize])\n",
    "    res = np.abs(cv2.filter2D(img, -1, kernel))\n",
    "    res = res/np.max(res)\n",
    "    y_kernel = np.array([[-1],[-1],[0],[1],[1]])\n",
    "    mask = np.abs(cv2.filter2D(img, -1, y_kernel))\n",
    "    binary = np.zeros_like(res)\n",
    "    binary[(res>=x_thresh[0])&(res<=x_thresh[1])&(mask<y_thresh[1])] = 1\n",
    "    d_kernel = np.ones((3,3),np.uint8)\n",
    "    binary = cv2.dilate(binary,d_kernel,iterations = 1)\n",
    "    binary = cv2.erode(binary,d_kernel,iterations = 1)\n",
    "    \n",
    "    n,n_blobs,stats,cent = cv2.connectedComponentsWithStats(binary.astype(np.uint8),\n",
    "                                                            connectivity=8,ltype=cv2.CV_32S)\n",
    "    tot = img.shape[0]*img.shape[1]\n",
    "    areas = stats[:,-1]\n",
    "    big_segs = heapq.nlargest(6, range(len(areas)), areas.take)\n",
    "    binary = np.zeros_like(res)\n",
    "    for i in big_segs[1:]:\n",
    "        if areas[i]>min_area: binary[n_blobs==i]=1\n",
    "    return binary\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x108a02ba8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAQUAAAD8CAYAAAB+fLH0AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUm4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1077ef828>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def region_of_interest(shape, vertices):\n",
    "    #defining a blank mask to start with\n",
    "    mask = np.zeros(shape,dtype=np.uint8)\n",
    "    cv2.fillPoly(mask, np.array([vertices], dtype=np.int32),1)\n",
    "    return mask\n",
    "roi = region_of_interest((720,1280), [[300,420],[200,670],[700,600],[1150,670],[1000,420]])\n",
    "roi = cv2.undistort(roi, mtx, dist, None, mtx)\n",
    "roi = cv2.warpPerspective(roi, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "plt.imshow(roi,cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "DEFAULT_LANE_WIDTH = 360\n",
    "POLY_FIT_THRES = 20\n",
    "CONSECUTIVE_THRES = 20 \n",
    "ERROR_CODE = {1: \"Success\",\n",
    "              -1:\"Lane width abnormal\",\n",
    "              -2:\"Polynomial fitting failed\",\n",
    "              -3:\"Lane boundary failed parallel check\",\n",
    "              -4:\"Lane deviated too much from the previous frame\"\n",
    "             }\n",
    "\n",
    "class Lane():\n",
    "    def __init__(self, img_size):\n",
    "        self.detected = False\n",
    "        self.img_size = img_size\n",
    "        self.width = DEFAULT_LANE_WIDTH # Lane width\n",
    "        # y coodinates, will be used to plot polynomial\n",
    "        self.ploty = np.linspace(0, img_size[0]-1, img_size[0]).astype('int64')\n",
    "        self.window_centroids = []\n",
    "        self.left_x = np.array([], dtype=np.int64)\n",
    "        self.left_y = np.array([], dtype=np.int64)\n",
    "        self.right_x = np.array([], dtype=np.int64)\n",
    "        self.right_y = np.array([], dtype=np.int64)\n",
    "        self.left_poly = []\n",
    "        self.right_poly = []\n",
    "        self.last_left_fitx = None\n",
    "        self.last_right_fitx = None\n",
    "        self.left_fitx = None\n",
    "        self.right_fitx = None\n",
    "        self.left_polyfit_err = 0 # err for left polynomial fitting.\n",
    "        self.right_polyfit_err = 0 # err for right polynomial fitting.\n",
    "        self.canvas = np.zeros((self.img_size[0],self.img_size[1],3),dtype=np.uint8) #used to draw the window box\n",
    "        \n",
    "    def reset_canvas(self):\n",
    "        self.canvas[:,:,:] = 0 # reset to blank\n",
    "        \n",
    "    def lane_check(self):\n",
    "        self.detected = False\n",
    "        if abs(self.width-DEFAULT_LANE_WIDTH)>50:\n",
    "            print(\"Lane width abnormal\")\n",
    "            return -1\n",
    "        \n",
    "        if self.left_polyfit_err>POLY_FIT_THRES or self.right_polyfit_err>POLY_FIT_THRES:\n",
    "            print(\"Polynomial fitting failed\")\n",
    "            return -2\n",
    "        \n",
    "        h_check = self.right_fitx-self.left_fitx\n",
    "        # Treat corner case where curve goes out of screen\n",
    "        h_check[(self.right_fitx==img_size[1])|(self.left_fitx==0)|\n",
    "                (self.right_fitx==0)|(self.left_fitx==img_size[1])] = DEFAULT_LANE_WIDTH\n",
    "        if h_check.min()<DEFAULT_LANE_WIDTH-50 or h_check.max()>DEFAULT_LANE_WIDTH+50:\n",
    "            print(\"Lane boundary failed parallel check\")\n",
    "            return -3\n",
    "        \n",
    "        left_dev = np.abs(self.last_left_fitx-self.left_fitx).max()\n",
    "        right_dev = np.abs(self.last_right_fitx-self.right_fitx).max()\n",
    "        if left_dev>CONSECUTIVE_THRES or right_dev>CONSECUTIVE_THRES:\n",
    "            print(\"Lane deviated too much from the previous frame\")\n",
    "            return -4\n",
    "        \n",
    "        self.detected = True\n",
    "        return 1\n",
    "        \n",
    "    def polyfit_left(self):\n",
    "        self.last_left_fitx = self.left_fitx\n",
    "        if self.left_x.size>0:\n",
    "            # Fit a second order polynomial\n",
    "            self.left_poly, err, _, _, _ = np.polyfit(self.left_y, self.left_x, 2, full=True)\n",
    "            self.left_polyfit_err = err/len(self.left_x)\n",
    "            self.left_fitx = (self.left_poly[0]*self.ploty**2 + \n",
    "                              self.left_poly[1]*self.ploty + \n",
    "                              self.left_poly[2]).astype('int64')\n",
    "            self.left_fitx[self.left_fitx<0]=0\n",
    "            self.left_fitx[self.left_fitx>img_size[0]-1]=img_size[0]-1\n",
    "                \n",
    "        return self.left_fitx\n",
    "            \n",
    "    def polyfit_right(self):\n",
    "        self.last_right_fitx = self.right_fitx\n",
    "        if self.right_x.size>0:\n",
    "            # Fit a second order polynomial\n",
    "            self.right_poly, err, _, _, _ = np.polyfit(self.right_y, self.right_x, 2, full=True)\n",
    "            self.right_polyfit_err = err/len(self.right_x)\n",
    "            self.right_fitx = (self.right_poly[0]*self.ploty**2 + \n",
    "                               self.right_poly[1]*self.ploty + \n",
    "                               self.right_poly[2]).astype('int64')\n",
    "            self.right_fitx[self.right_fitx<0]=0\n",
    "            self.right_fitx[self.right_fitx>img_size[0]-1]=img_size[0]-1\n",
    "                \n",
    "        return self.right_fitx\n",
    "    \n",
    "    def draw_lane(self):\n",
    "        # Draw the lane pixels and lane curve\n",
    "        # Make right lane pixels green, left lane red\n",
    "        self.canvas[self.left_y,self.left_x,0] = 255\n",
    "        self.canvas[self.right_y,self.right_x,1] = 255\n",
    "        # Draw polynomial\n",
    "        self.canvas[self.ploty,left_fitx,0:2] = 255\n",
    "        self.canvas[self.ploty,right_fitx,0:2] = 255\n",
    "        return self.canvas\n",
    "    \n",
    "    def compute_curvature(self):\n",
    "        return\n",
    "    \n",
    "    def overlay_curvature(self):\n",
    "        return\n",
    "\n",
    "lane = Lane(img_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def window_mask(width, height, img_ref, center,level):\n",
    "    output = np.zeros_like(img_ref)\n",
    "    output[int(img_ref.shape[0]-(level+1)*height):int(img_ref.shape[0]-level*height),\n",
    "           max(0,int(center-width/2)):min(int(center+width/2),img_ref.shape[1])] = 1\n",
    "    return output\n",
    "\n",
    "def initial_window_finder(images, thresholds, window_width,\n",
    "                          left_bound, right_bound, \n",
    "                          upper_bound, lower_bound):\n",
    "    window = np.ones(window_width) # Define window template\n",
    "    for i, (image, threshold) in enumerate(zip(images,thresholds)):\n",
    "        # Sum the bottom of the image with the given height to get slice\n",
    "        v_sum = np.sum(image[upper_bound:lower_bound,left_bound:right_bound], axis=0)\n",
    "        # Find the starting position for a lane by using np.sum to get the vertical image slice\n",
    "        # and then np.convolve the vertical image slice with the window template\n",
    "        conv = np.convolve(window,v_sum)\n",
    "        m = conv.max()\n",
    "        noise_check = np.zeros_like(conv)\n",
    "        noise_check[conv>m/2] = 1 \n",
    "        # the peak should have enough signals, and should stand out from the rests (not noise).\n",
    "        if m>threshold and noise_check.sum()<noise_check.size/4:\n",
    "            center = int(np.argmax(conv)-window.size/2 + left_bound)\n",
    "            return (i, center)\n",
    "\n",
    "    return (0, -1)\n",
    "\n",
    "def find_window_per_level(channels, l_ind, r_ind, l_center, r_center, level,\n",
    "                          window_width, window_height, margin, thresholds, lane):\n",
    "    # convolve the window into the vertical slice of the image\n",
    "    l_ind_bg, r_ind_bg = l_ind, r_ind\n",
    "    l_status, r_status = -1, -1 #not started yet, 0 started, 1 finished\n",
    "    N = len(channels)\n",
    "    window = np.ones(window_width) # Define window template\n",
    "    # Use window_width/2 as offset because convolution signal reference is at right side of window, \n",
    "    # not center of window\n",
    "    offset = window_width/2\n",
    "    while l_status<1:\n",
    "        l_image = channels[l_ind]\n",
    "        l_image_layer = np.sum(l_image[int(l_image.shape[0]-(level+1)*window_height):\n",
    "                                       int(l_image.shape[0]-level*window_height),\n",
    "                                       0:int(l_image.shape[1]/2)], \n",
    "                               axis=0)\n",
    "        l_conv_signal = np.convolve(window, l_image_layer)\n",
    "        # Find the best left centroid by using past left center as a reference\n",
    "        l_min_index = int(max(l_center+offset-margin,0))\n",
    "        l_max_index = int(min(l_center+offset+margin,l_image_layer.size))\n",
    "        l_range = l_conv_signal[l_min_index:l_max_index]\n",
    "        l_peak_idx = np.argmax(l_range)\n",
    "        \n",
    "        # Update window center only if pixels in that slot exceed our threshold\n",
    "        if (l_range[l_peak_idx]>thresholds[l_ind] and \n",
    "            len(np.where(l_conv_signal>l_range[l_peak_idx]/2))<l_conv_signal.size/2):\n",
    "            l_center = l_peak_idx+l_min_index-offset\n",
    "            l_status = 1\n",
    "#             cv2.rectangle(lane.canvas[:,:,2], # show window\n",
    "#                           (max(0,int(l_center-window_width/2)),\n",
    "#                            int(l_image.shape[0]-(level+1)*window_height)),\n",
    "#                           (min(int(l_center+window_width/2),l_image.shape[1]),\n",
    "#                            int(l_image.shape[0]-level*window_height)),\n",
    "#                           255,3)\n",
    "        else:\n",
    "            l_ind += 1\n",
    "            l_ind = l_ind%N\n",
    "            l_status = 1 if l_ind==l_ind_bg else 0\n",
    "          \n",
    "    while r_status<1:\n",
    "        r_image = channels[r_ind] # use the last channel which has the right lane edge.\n",
    "        left_boundary = int(r_image.shape[1]/2)\n",
    "        r_image_layer = np.sum(r_image[int(r_image.shape[0]-(level+1)*window_height):\n",
    "                                       int(r_image.shape[0]-level*window_height),\n",
    "                                       left_boundary:], \n",
    "                               axis=0)\n",
    "        # Find the best right centroid by using past right center as a reference\n",
    "        r_min_index = int(max(r_center-left_boundary+offset-margin,0))\n",
    "        r_max_index = int(min(r_center-left_boundary+offset+margin,r_image_layer.size))\n",
    "        r_conv_signal = np.convolve(window, r_image_layer)\n",
    "        r_range = r_conv_signal[r_min_index:r_max_index]\n",
    "        r_peak_idx = np.argmax(r_range)\n",
    "        if (r_range[r_peak_idx]>threshold[r_ind] and \n",
    "            len(np.where(r_conv_signal>r_range[r_peak_idx]/2))<r_conv_signal.size/2):\n",
    "            r_center = r_peak_idx+left_boundary+r_min_index-offset\n",
    "            r_status = 1\n",
    "#             cv2.rectangle(lane.canvas[:,:,2], # show window\n",
    "#                           (max(0,int(r_center-window_width/2)),\n",
    "#                            int(r_image.shape[0]-(level+1)*window_height)),\n",
    "#                           (min(int(r_center+window_width/2),r_image.shape[1]),\n",
    "#                            int(r_image.shape[0]-level*window_height)),\n",
    "#                           255,3)\n",
    "        else:\n",
    "            r_ind += 1\n",
    "            r_ind = r_ind%N\n",
    "            r_status = 1 if r_ind==r_ind_bg else 0\n",
    "            \n",
    "    return (l_ind, l_center, r_ind, r_center)\n",
    "\n",
    "def get_points_in_window(channels, level,\n",
    "                         l_ind, r_ind, l_center, r_center, \n",
    "                         window_width = 50, window_height = 80):\n",
    "    l_image = np.maximum(channels[0],channels[l_ind])\n",
    "    if l_center>=0:\n",
    "        l_mask = window_mask(window_width,window_height,l_image,l_center,level)\n",
    "        l_points = l_image*l_mask\n",
    "\n",
    "    r_image = np.maximum(channels[0],channels[r_ind])\n",
    "    if r_center>=0:\n",
    "        r_mask = window_mask(window_width,window_height,r_image,r_center,level)\n",
    "        r_points = r_image*r_mask\n",
    "        \n",
    "    return (l_points, r_points)\n",
    "\n",
    "def init_lane_finder(channels, thresholds, lane, window_width = 50, window_height = 80, \n",
    "               margin = 100):\n",
    "    M, N = channels[0].shape\n",
    "    (l_ind, l_center) = initial_window_finder(channels,thresholds,window_width,\n",
    "                                              int(N/8),int(N*3/8), \n",
    "                                              int(M*3/4), M)\n",
    "    (r_ind, r_center) = initial_window_finder(channels,thresholds,window_width,\n",
    "                                              int(N*5/8), int(N*7/8), \n",
    "                                              int(M*3/4), M)\n",
    "    \n",
    "    if l_center==-1 and r_center!=-1:\n",
    "        (l_ind, l_center) = (0, r_center-lane.width)\n",
    "    if r_center==-1 and l_center!=-1:\n",
    "        (r_ind, r_center) = (0, l_center+lane.width)\n",
    "    \n",
    "    lane.window_centroids = [(l_ind, l_center, r_ind, r_center)]\n",
    "    \n",
    "    l_points, r_points = np.zeros((M, N)), np.zeros((M, N))\n",
    "    for level in range(1,(int)(M/window_height)):\n",
    "        l_ind, l_center, r_ind, r_center = find_window_per_level(channels, l_ind, r_ind, l_center, r_center, \n",
    "                                                                 level, window_width, window_height, \n",
    "                                                                 margin, thresholds, lane)\n",
    "        \n",
    "        lane.window_centroids.append((l_ind, l_center, r_ind, r_center))\n",
    "        # Points used to draw all the left and right windows\n",
    "        (l_layer_points,r_layer_points) = get_points_in_window(channels, level, \n",
    "                                                               l_ind, r_ind, l_center, r_center, \n",
    "                                                               window_width, window_height)\n",
    "        l_points = np.maximum(l_points,l_layer_points)\n",
    "        r_points = np.maximum(r_points,r_layer_points)\n",
    "        \n",
    "    (lane.left_y, lane.left_x) = np.where(l_points>0)\n",
    "    (lane.right_y, lane.right_x) = np.where(r_points>0)\n",
    "    lane.polyfit_left()\n",
    "    lane.polyfit_right()\n",
    "    \n",
    "    return lane.lane_check()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 720)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lane.canvas[:,:,2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def channel_decompose(img, saturation_white_thresh=(0, 2), \n",
    "             saturation_yellow_thresh=(100, 180), \n",
    "             hue_thresh=(18, 25), value_thresh=(220, 255),\n",
    "             component_limit=6, min_area=1000,\n",
    "             ksize=15):\n",
    "    \n",
    "    r_channel = img[:,:,0]\n",
    "    img = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "\n",
    "    # Convert to HLS color space and separate the V channel\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_RGB2HSV).astype(np.float)\n",
    "    h_channel = hsv[:,:,0]\n",
    "    s_channel = hsv[:,:,1]\n",
    "    v_channel = hsv[:,:,2]\n",
    "    \n",
    "    r_binary = topo_parabolic_bump_filter(cv2.GaussianBlur(r_channel,(5,5),0),\n",
    "                                          x_thresh=(0.9,1), min_area=1000, ksize=ksize)\n",
    "    r_perspective = cv2.warpPerspective(r_binary, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    v_binary = np.zeros_like(v_channel)\n",
    "    v_binary[(v_channel>=value_thresh[0])&(v_channel<=value_thresh[1])] = 1\n",
    "    v_perspective = cv2.warpPerspective(v_binary, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    h_binary = np.zeros_like(h_channel)\n",
    "    h_binary[(h_channel>=hue_thresh[0])&(h_channel<=hue_thresh[1])] = 1\n",
    "    h_perspective = cv2.warpPerspective(h_binary, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    sw_binary = np.zeros_like(s_channel)\n",
    "    sw_binary[(s_channel>=saturation_white_thresh[0])&(s_channel<=saturation_white_thresh[1])] = 1\n",
    "    sw_perspective = cv2.warpPerspective(sw_binary, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    sy_binary = np.zeros_like(s_channel)\n",
    "    sy_binary[(s_channel>=saturation_yellow_thresh[0])&(s_channel<=saturation_yellow_thresh[1])] = 1\n",
    "    sy_perspective = cv2.warpPerspective(sy_binary, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    \n",
    "    return (v_perspective,r_perspective,h_perspective,sw_perspective,sy_perspective)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pipeline(img, lane, mtx, dist, saturation_white_thresh=(0, 2), \n",
    "             saturation_yellow_thresh=(100, 180), \n",
    "             hue_thresh=(18, 25), value_thresh=(220, 255),\n",
    "             component_limit=6, min_area=1000,\n",
    "             ksize=15, mask=None):\n",
    "    \n",
    "    lane.reset_canvas()\n",
    "    if mask is None: mask = np.ones_like(perspective)\n",
    "    channels = channel_decompose(img, saturation_white_thresh, \n",
    "                                 saturation_yellow_thresh, \n",
    "                                 hue_thresh, value_thresh,\n",
    "                                 component_limit, min_area,\n",
    "                                 ksize)\n",
    "    channels = [img*mask for img in channels] #ignore outside of masked region\n",
    "    thresholds = [40,40,40,5,5] #set threshold for each channel\n",
    "        \n",
    "    init_lane_finder(channels, thresholds, lane, window_width = 50, window_height = 80, \n",
    "               margin = 100)\n",
    "        \n",
    "    res = laneFinder(perspective, l_center, r_center, lane, window_width = 50, \n",
    "                     window_height = 80, margin = 100)\n",
    "\n",
    "    return lane.draw_lane()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'threshold' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-7a8a0a7b5248>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"test_images/straight_lines1.jpg\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mtest_img\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_img\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlane\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mroi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-b1cee79d96b5>\u001b[0m in \u001b[0;36mpipeline\u001b[0;34m(img, lane, mtx, dist, saturation_white_thresh, saturation_yellow_thresh, hue_thresh, value_thresh, component_limit, min_area, ksize, mask)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     init_lane_finder(channels, thresholds, lane, window_width = 50, window_height = 80, \n\u001b[0;32m---> 18\u001b[0;31m                margin = 100)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     res = laneFinder(perspective, l_center, r_center, lane, window_width = 50, \n",
      "\u001b[0;32m<ipython-input-31-a915137dd420>\u001b[0m in \u001b[0;36minit_lane_finder\u001b[0;34m(channels, thresholds, lane, window_width, window_height, margin)\u001b[0m\n\u001b[1;32m    130\u001b[0m         l_ind, l_center, r_ind, r_center = find_window_per_level(channels, l_ind, r_ind, l_center, r_center, \n\u001b[1;32m    131\u001b[0m                                                                  \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_width\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwindow_height\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                                                                  margin, thresholds, lane)\n\u001b[0m\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m         \u001b[0mlane\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow_centroids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml_center\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_ind\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr_center\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-31-a915137dd420>\u001b[0m in \u001b[0;36mfind_window_per_level\u001b[0;34m(channels, l_ind, r_ind, l_center, r_center, level, window_width, window_height, margin, thresholds, lane)\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mr_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_conv_signal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr_min_index\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mr_max_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mr_peak_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr_range\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m         if (r_range[r_peak_idx]>threshold[r_ind] and \n\u001b[0m\u001b[1;32m     80\u001b[0m             len(np.where(r_conv_signal>r_range[r_peak_idx]/2))<r_conv_signal.size/2):\n\u001b[1;32m     81\u001b[0m             \u001b[0mr_center\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mr_peak_idx\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mleft_boundary\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mr_min_index\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'threshold' is not defined"
     ]
    }
   ],
   "source": [
    "test_img = cv2.imread(\"test_images/straight_lines1.jpg\")\n",
    "test_img = cv2.cvtColor(test_img, cv2.COLOR_BGR2RGB)\n",
    "res = pipeline(test_img, lane, mtx, dist, mask=roi)\n",
    "plt.imshow(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video challenge_video_poly_pipeline_output.mp4\n",
      "[MoviePy] Writing video challenge_video_poly_pipeline_output.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 485/485 [01:28<00:00,  5.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: challenge_video_poly_pipeline_output.mp4 \n",
      "\n",
      "CPU times: user 1min 22s, sys: 17.8 s, total: 1min 40s\n",
      "Wall time: 1min 28s\n"
     ]
    }
   ],
   "source": [
    "challenge_output = 'challenge_video_poly_pipeline_output.mp4'\n",
    "processor = lambda x:pipeline(x, lane, mtx, dist)\n",
    "\n",
    "clip2 = VideoFileClip('challenge_video.mp4')\n",
    "challenge_clip = clip2.fl_image(processor)\n",
    "%time challenge_clip.write_videofile(challenge_output, audio=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"challenge_video_green.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(challenge_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Computer Perspective Tranformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_size = (800,1280)\n",
    "\n",
    "def perspective_transformation(target_img_size):\n",
    "    src = np.array([[592, 453], \n",
    "                    [695,453],\n",
    "                    [970, 630],\n",
    "                    [348, 630]], dtype = \"float32\")\n",
    "\n",
    "    h_margin = 200\n",
    "    top_margin = 300\n",
    "    bottom_margin = 50\n",
    "    dst = np.array([[h_margin, top_margin], \n",
    "                    [target_img_size[0] - h_margin, top_margin],\n",
    "                    [target_img_size[0] - h_margin, target_img_size[1] - bottom_margin],\n",
    "                    [h_margin, target_img_size[1] - bottom_margin]], dtype = \"float32\")\n",
    "    return cv2.getPerspectiveTransform(src, dst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def abs_sobel_thresh(img, orient='x', thresh_min=0, thresh_max=255):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    if orient == 'x':\n",
    "        sb = cv2.Sobel(gray, cv2.CV_64F, 1, 0)\n",
    "    if orient == 'y':\n",
    "        sb = cv2.Sobel(gray, cv2.CV_64F, 0, 1)\n",
    "    abs_sb = np.absolute(sb)\n",
    "    sc_sb = np.uint8(abs_sb*255/np.max(abs_sb))\n",
    "    binary_output = np.zeros_like(sc_sb)\n",
    "    binary_output[(sc_sb>thresh_min)&(sc_sb<thresh_max)]=1\n",
    "    return binary_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_lane(original_img, binary_img, l_fit, r_fit, Minv):\n",
    "    new_img = np.copy(original_img)\n",
    "    if l_fit is None or r_fit is None:\n",
    "        return original_img\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(binary_img).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    h,w = binary_img.shape\n",
    "    ploty = np.linspace(0, h-1, num=h)# to cover same y-range as image\n",
    "    left_fitx = l_fit[0]*ploty**2 + l_fit[1]*ploty + l_fit[2]\n",
    "    right_fitx = r_fit[0]*ploty**2 + r_fit[1]*ploty + r_fit[2]\n",
    "\n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "    cv2.polylines(color_warp, np.int32([pts_left]), isClosed=False, color=(255,0,255), thickness=15)\n",
    "    cv2.polylines(color_warp, np.int32([pts_right]), isClosed=False, color=(0,255,255), thickness=15)\n",
    "\n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (w, h)) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(new_img, 1, newwarp, 0.5, 0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def draw_result(original_img, radius, center_dist):\n",
    "    new_img = np.copy(original_img)\n",
    "    h = new_img.shape[0]\n",
    "    font = cv2.FONT_HERSHEY_DUPLEX\n",
    "    text = 'Curve radius: ' + '{:04.2f}'.format(radius) + 'm'\n",
    "    cv2.putText(new_img, text, (40,70), font, 1.5, (200,255,155), 2, cv2.LINE_AA)\n",
    "    direction = ''\n",
    "    if center_dist > 0:\n",
    "        direction = 'right'\n",
    "    elif center_dist < 0:\n",
    "        direction = 'left'\n",
    "    abs_center_dist = abs(center_dist)\n",
    "    text = '{:04.3f}'.format(abs_center_dist) + 'm ' + direction + ' of center'\n",
    "    cv2.putText(new_img, text, (40,120), font, 1.5, (200,255,155), 2, cv2.LINE_AA)\n",
    "    return new_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "exampleImg_out2 = draw_data(exampleImg_out1, (rad_l+rad_r)/2, d_center)\n",
    "plt.imshow(exampleImg_out2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
